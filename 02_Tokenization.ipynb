{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87a0a82b",
   "metadata": {},
   "source": [
    "### Tokenization:-\n",
    "\n",
    "1. Tokenization is a crucial process in Natural Language Processing (NLP) that involves breaking down a piece of text into smaller units called tokens. \n",
    "2. These tokens can be words, parts of words, or even characters like punctuation . \n",
    "3. The primary goal of tokenization is to represent text in a manner that's meaningful for machines without losing its context.\n",
    "4. It turns an unstructured string (text document) into a numerical data structure suitable for machine learning .\n",
    "5. The motivation behind tokenization is to present the computer with some finite set of symbols that it can combine to produce the desired result. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5c3ff4",
   "metadata": {},
   "source": [
    "***all imports are taken from:  from nltk.tokenize import ......***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09162c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = '''Hello friends!\n",
    "How are you? Welcome to python programming.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5812528b",
   "metadata": {},
   "source": [
    "### 1. Sentence Tokenization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5352abc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello friends!', 'How are you?', 'Welcome to python programming.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sent_token = sent_tokenize(sents)   # it will return list of sentences from the text\n",
    "\n",
    "sent_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ce19d9",
   "metadata": {},
   "source": [
    "### 2. Word Tokenization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12cb4d41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'friends',\n",
       " '!',\n",
       " 'How',\n",
       " 'are',\n",
       " 'you',\n",
       " '?',\n",
       " 'Welcome',\n",
       " 'to',\n",
       " 'python',\n",
       " 'programming',\n",
       " '.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "word_token = word_tokenize(sents)  # it will return the list if each word as individual even punctuations as word\n",
    "\n",
    "word_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee9b913",
   "metadata": {},
   "source": [
    "### 3. White Space Tokenization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0905cf36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'friends!',\n",
       " 'How',\n",
       " 'are',\n",
       " 'you?',\n",
       " 'Welcome',\n",
       " 'to',\n",
       " 'python',\n",
       " 'programming.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "\n",
    "white_token = WhitespaceTokenizer()   #it is a fun\n",
    "\n",
    "white_token.tokenize(sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43170dec",
   "metadata": {},
   "source": [
    "### 4. Space Tokenization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e514f287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'friends!\\nHow',\n",
       " 'are',\n",
       " 'you?',\n",
       " 'Welcome',\n",
       " 'to',\n",
       " 'python',\n",
       " 'programming.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import SpaceTokenizer\n",
    "\n",
    "sp_token = SpaceTokenizer()\n",
    "\n",
    "sp_token.tokenize(sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd572197",
   "metadata": {},
   "source": [
    "### 5. Line Tokenization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d1f53db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello friends!', 'How are you? Welcome to python programming.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import LineTokenizer\n",
    "\n",
    "ln_token = LineTokenizer()    #it will tokenize at '\\n' new line and return list \n",
    "\n",
    "ln_token.tokenize(sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f33cd0e",
   "metadata": {},
   "source": [
    "### 6. Tab Tokenization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66fb61d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello ', 'friends!\\nHow are you? Welcome to', ' python programming.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent1 = '''Hello \\tfriends!\n",
    "How are you? Welcome to\\t python programming.'''\n",
    "\n",
    "from nltk.tokenize import TabTokenizer\n",
    "\n",
    "tab_token = TabTokenizer()    # return the list of tab seperated part\n",
    "\n",
    "tab_token.tokenize(sent1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f54de9",
   "metadata": {},
   "source": [
    "### 7. Multi-word Tokenization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e16c5d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Van Rossom',\n",
       " 'is',\n",
       " 'in',\n",
       " 'pune',\n",
       " 'today',\n",
       " '.',\n",
       " 'We',\n",
       " 'welcomed',\n",
       " 'Van Rossom',\n",
       " 'here']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = 'Van Rossom is in pune today. We welcomed Van Rossom here'\n",
    "\n",
    "from nltk.tokenize import MWETokenizer\n",
    "\n",
    "mwt_token = MWETokenizer(separator = ' ')\n",
    "\n",
    "mwt_token.add_mwe(('Van', 'Rossom'))\n",
    "\n",
    "token = mwt_token.tokenize(word_tokenize(sent))\n",
    "\n",
    "token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8905c4a",
   "metadata": {},
   "source": [
    "### 8. Tweet Tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21fafb1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'friends',\n",
       " '!',\n",
       " 'üòÇ',\n",
       " ':',\n",
       " '!',\n",
       " 'How',\n",
       " 'are',\n",
       " 'you',\n",
       " '?',\n",
       " 'Welcome',\n",
       " 'to',\n",
       " '‚ò†',\n",
       " 'Ô∏èpython',\n",
       " 'programming',\n",
       " 'ü´Ä',\n",
       " 'ü´Å',\n",
       " 'üß†',\n",
       " '.',\n",
       " ':D',\n",
       " 'Check',\n",
       " 'my',\n",
       " 'web',\n",
       " ':',\n",
       " 'https://python.org']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "sent2 = '''Hello friends! üòÇ :!\n",
    "How are you? Welcome to ‚ò†Ô∏èpython programming ü´Ä ü´Å üß†.:D\n",
    "Check my web: https://python.org'''\n",
    "\n",
    "tk = TweetTokenizer()\n",
    "\n",
    "tk.tokenize(sent2)    ## it will trturn list with emojis as well"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa2b890",
   "metadata": {},
   "source": [
    "### 9. How to make our own tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed4fa075",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def custom_tokenizer(text):\n",
    "    return re.split(r\"[.,;?!\\s]+\",text)    # it will split the string whenever mentioned characters occure..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "123c846c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'friends',\n",
       " 'How',\n",
       " 'are',\n",
       " 'you',\n",
       " 'Welcome',\n",
       " 'to',\n",
       " 'python',\n",
       " 'programming',\n",
       " '']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_tokenizer(sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb36753",
   "metadata": {},
   "source": [
    "                                                                                              Thank you :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
